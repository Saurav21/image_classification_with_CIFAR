{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR_10_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSIaRmedcXh9"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVfaxOyoLeVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c8538a-c835-44bd-b5b8-3e984a29cee1"
      },
      "source": [
        "# importing CIFAR datasets from keras\n",
        "\n",
        "(x_train,y_train),(x_test,y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mApgVHrp8ls3",
        "outputId": "7de4bd7c-8440-4d0c-fb4c-d2e6435aca7b"
      },
      "source": [
        "x_train.shape,y_train.shape,x_test.shape,y_test.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 32, 32, 3), (50000, 1), (10000, 32, 32, 3), (10000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "qGaBn-lj8nn7",
        "outputId": "febb2e46-6e9c-48a2-d2b3-2a7f0be43144"
      },
      "source": [
        "import  matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(x_train[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fdde05f3390>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WNn-0rh8nlY",
        "outputId": "ce8f35d4-c948-402d-863b-ff55a6b528c4"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xnW2DiJAQyE"
      },
      "source": [
        "##**Processing the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLSu8L978ngA",
        "outputId": "9ae7bb2a-f958-47cc-c3dc-8310b317e997"
      },
      "source": [
        "x_train.dtype, y_train.dtype, x_test.dtype, y_test.dtype"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dtype('uint8'), dtype('uint8'), dtype('uint8'), dtype('uint8'))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qFF_BLy8ndk",
        "outputId": "7e8f86f1-4da7-4465-9be6-f91fb13d0d28"
      },
      "source": [
        "x_train.astype(\"float32\")\n",
        "x_test.astype(\"float32\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[158., 112.,  49.],\n",
              "         [159., 111.,  47.],\n",
              "         [165., 116.,  51.],\n",
              "         ...,\n",
              "         [137.,  95.,  36.],\n",
              "         [126.,  91.,  36.],\n",
              "         [116.,  85.,  33.]],\n",
              "\n",
              "        [[152., 112.,  51.],\n",
              "         [151., 110.,  40.],\n",
              "         [159., 114.,  45.],\n",
              "         ...,\n",
              "         [136.,  95.,  31.],\n",
              "         [125.,  91.,  32.],\n",
              "         [119.,  88.,  34.]],\n",
              "\n",
              "        [[151., 110.,  47.],\n",
              "         [151., 109.,  33.],\n",
              "         [158., 111.,  36.],\n",
              "         ...,\n",
              "         [139.,  98.,  34.],\n",
              "         [130.,  95.,  34.],\n",
              "         [120.,  89.,  33.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 68., 124., 177.],\n",
              "         [ 42., 100., 148.],\n",
              "         [ 31.,  88., 137.],\n",
              "         ...,\n",
              "         [ 38.,  97., 146.],\n",
              "         [ 13.,  64., 108.],\n",
              "         [ 40.,  85., 127.]],\n",
              "\n",
              "        [[ 61., 116., 168.],\n",
              "         [ 49., 102., 148.],\n",
              "         [ 35.,  85., 132.],\n",
              "         ...,\n",
              "         [ 26.,  82., 130.],\n",
              "         [ 29.,  82., 126.],\n",
              "         [ 20.,  64., 107.]],\n",
              "\n",
              "        [[ 54., 107., 160.],\n",
              "         [ 56., 105., 149.],\n",
              "         [ 45.,  89., 132.],\n",
              "         ...,\n",
              "         [ 24.,  77., 124.],\n",
              "         [ 34.,  84., 129.],\n",
              "         [ 21.,  67., 110.]]],\n",
              "\n",
              "\n",
              "       [[[235., 235., 235.],\n",
              "         [231., 231., 231.],\n",
              "         [232., 232., 232.],\n",
              "         ...,\n",
              "         [233., 233., 233.],\n",
              "         [233., 233., 233.],\n",
              "         [232., 232., 232.]],\n",
              "\n",
              "        [[238., 238., 238.],\n",
              "         [235., 235., 235.],\n",
              "         [235., 235., 235.],\n",
              "         ...,\n",
              "         [236., 236., 236.],\n",
              "         [236., 236., 236.],\n",
              "         [235., 235., 235.]],\n",
              "\n",
              "        [[237., 237., 237.],\n",
              "         [234., 234., 234.],\n",
              "         [234., 234., 234.],\n",
              "         ...,\n",
              "         [235., 235., 235.],\n",
              "         [235., 235., 235.],\n",
              "         [234., 234., 234.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 87.,  99.,  89.],\n",
              "         [ 43.,  51.,  37.],\n",
              "         [ 19.,  23.,  11.],\n",
              "         ...,\n",
              "         [169., 184., 179.],\n",
              "         [182., 197., 193.],\n",
              "         [188., 202., 201.]],\n",
              "\n",
              "        [[ 82.,  96.,  82.],\n",
              "         [ 46.,  57.,  36.],\n",
              "         [ 36.,  44.,  22.],\n",
              "         ...,\n",
              "         [174., 189., 183.],\n",
              "         [185., 200., 196.],\n",
              "         [187., 202., 200.]],\n",
              "\n",
              "        [[ 85., 101.,  83.],\n",
              "         [ 62.,  75.,  48.],\n",
              "         [ 58.,  67.,  38.],\n",
              "         ...,\n",
              "         [168., 183., 178.],\n",
              "         [180., 195., 191.],\n",
              "         [186., 200., 199.]]],\n",
              "\n",
              "\n",
              "       [[[158., 190., 222.],\n",
              "         [158., 187., 218.],\n",
              "         [139., 166., 194.],\n",
              "         ...,\n",
              "         [228., 231., 234.],\n",
              "         [237., 239., 243.],\n",
              "         [238., 241., 246.]],\n",
              "\n",
              "        [[170., 200., 229.],\n",
              "         [172., 199., 226.],\n",
              "         [151., 176., 201.],\n",
              "         ...,\n",
              "         [232., 232., 236.],\n",
              "         [246., 246., 250.],\n",
              "         [246., 247., 251.]],\n",
              "\n",
              "        [[174., 201., 225.],\n",
              "         [176., 200., 222.],\n",
              "         [157., 179., 199.],\n",
              "         ...,\n",
              "         [230., 229., 232.],\n",
              "         [250., 249., 251.],\n",
              "         [245., 244., 247.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 31.,  40.,  45.],\n",
              "         [ 30.,  39.,  44.],\n",
              "         [ 26.,  35.,  40.],\n",
              "         ...,\n",
              "         [ 37.,  40.,  46.],\n",
              "         [  9.,  13.,  14.],\n",
              "         [  4.,   7.,   5.]],\n",
              "\n",
              "        [[ 23.,  34.,  39.],\n",
              "         [ 27.,  38.,  43.],\n",
              "         [ 25.,  36.,  41.],\n",
              "         ...,\n",
              "         [ 19.,  20.,  24.],\n",
              "         [  4.,   6.,   3.],\n",
              "         [  5.,   7.,   3.]],\n",
              "\n",
              "        [[ 28.,  41.,  47.],\n",
              "         [ 30.,  43.,  50.],\n",
              "         [ 32.,  45.,  52.],\n",
              "         ...,\n",
              "         [  5.,   6.,   8.],\n",
              "         [  4.,   5.,   3.],\n",
              "         [  7.,   8.,   7.]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[ 20.,  15.,  12.],\n",
              "         [ 19.,  14.,  11.],\n",
              "         [ 15.,  14.,  11.],\n",
              "         ...,\n",
              "         [ 10.,   9.,   7.],\n",
              "         [ 12.,  11.,   9.],\n",
              "         [ 13.,  12.,  10.]],\n",
              "\n",
              "        [[ 21.,  16.,  13.],\n",
              "         [ 20.,  16.,  13.],\n",
              "         [ 18.,  17.,  12.],\n",
              "         ...,\n",
              "         [ 10.,   9.,   7.],\n",
              "         [ 10.,   9.,   7.],\n",
              "         [ 12.,  11.,   9.]],\n",
              "\n",
              "        [[ 21.,  16.,  13.],\n",
              "         [ 21.,  17.,  12.],\n",
              "         [ 20.,  18.,  11.],\n",
              "         ...,\n",
              "         [ 12.,  11.,   9.],\n",
              "         [ 12.,  11.,   9.],\n",
              "         [ 13.,  12.,  10.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 33.,  25.,  13.],\n",
              "         [ 34.,  26.,  15.],\n",
              "         [ 34.,  26.,  15.],\n",
              "         ...,\n",
              "         [ 28.,  25.,  52.],\n",
              "         [ 29.,  25.,  58.],\n",
              "         [ 23.,  20.,  42.]],\n",
              "\n",
              "        [[ 33.,  25.,  14.],\n",
              "         [ 34.,  26.,  15.],\n",
              "         [ 34.,  26.,  15.],\n",
              "         ...,\n",
              "         [ 27.,  24.,  52.],\n",
              "         [ 27.,  24.,  56.],\n",
              "         [ 25.,  22.,  47.]],\n",
              "\n",
              "        [[ 31.,  23.,  12.],\n",
              "         [ 32.,  24.,  13.],\n",
              "         [ 33.,  25.,  14.],\n",
              "         ...,\n",
              "         [ 24.,  23.,  50.],\n",
              "         [ 26.,  23.,  53.],\n",
              "         [ 25.,  20.,  47.]]],\n",
              "\n",
              "\n",
              "       [[[ 25.,  40.,  12.],\n",
              "         [ 15.,  36.,   3.],\n",
              "         [ 23.,  41.,  18.],\n",
              "         ...,\n",
              "         [ 61.,  82.,  78.],\n",
              "         [ 92., 113., 112.],\n",
              "         [ 75.,  89.,  92.]],\n",
              "\n",
              "        [[ 12.,  25.,   6.],\n",
              "         [ 20.,  37.,   7.],\n",
              "         [ 24.,  36.,  15.],\n",
              "         ...,\n",
              "         [115., 134., 138.],\n",
              "         [149., 168., 177.],\n",
              "         [104., 117., 131.]],\n",
              "\n",
              "        [[ 12.,  25.,  11.],\n",
              "         [ 15.,  29.,   6.],\n",
              "         [ 34.,  40.,  24.],\n",
              "         ...,\n",
              "         [154., 172., 182.],\n",
              "         [157., 175., 192.],\n",
              "         [116., 129., 151.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[100., 129.,  81.],\n",
              "         [103., 132.,  84.],\n",
              "         [104., 134.,  86.],\n",
              "         ...,\n",
              "         [ 97., 128.,  84.],\n",
              "         [ 98., 126.,  84.],\n",
              "         [ 91., 121.,  79.]],\n",
              "\n",
              "        [[103., 132.,  83.],\n",
              "         [104., 131.,  83.],\n",
              "         [107., 135.,  87.],\n",
              "         ...,\n",
              "         [101., 132.,  87.],\n",
              "         [ 99., 127.,  84.],\n",
              "         [ 92., 121.,  79.]],\n",
              "\n",
              "        [[ 95., 126.,  78.],\n",
              "         [ 95., 123.,  76.],\n",
              "         [101., 128.,  81.],\n",
              "         ...,\n",
              "         [ 93., 124.,  80.],\n",
              "         [ 95., 123.,  81.],\n",
              "         [ 92., 120.,  80.]]],\n",
              "\n",
              "\n",
              "       [[[ 73.,  78.,  75.],\n",
              "         [ 98., 103., 113.],\n",
              "         [ 99., 106., 114.],\n",
              "         ...,\n",
              "         [135., 150., 152.],\n",
              "         [135., 149., 154.],\n",
              "         [203., 215., 223.]],\n",
              "\n",
              "        [[ 69.,  73.,  70.],\n",
              "         [ 84.,  89.,  97.],\n",
              "         [ 68.,  75.,  81.],\n",
              "         ...,\n",
              "         [ 85.,  95.,  89.],\n",
              "         [ 71.,  82.,  80.],\n",
              "         [120., 133., 135.]],\n",
              "\n",
              "        [[ 69.,  73.,  70.],\n",
              "         [ 90.,  95., 100.],\n",
              "         [ 62.,  71.,  74.],\n",
              "         ...,\n",
              "         [ 74.,  81.,  70.],\n",
              "         [ 53.,  62.,  54.],\n",
              "         [ 62.,  74.,  69.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[123., 128.,  96.],\n",
              "         [132., 132., 102.],\n",
              "         [129., 128., 100.],\n",
              "         ...,\n",
              "         [108., 107.,  88.],\n",
              "         [ 62.,  60.,  55.],\n",
              "         [ 27.,  27.,  28.]],\n",
              "\n",
              "        [[115., 121.,  91.],\n",
              "         [123., 124.,  95.],\n",
              "         [129., 126.,  99.],\n",
              "         ...,\n",
              "         [115., 116.,  94.],\n",
              "         [ 66.,  65.,  59.],\n",
              "         [ 27.,  27.,  27.]],\n",
              "\n",
              "        [[116., 120.,  90.],\n",
              "         [121., 122.,  94.],\n",
              "         [129., 128., 101.],\n",
              "         ...,\n",
              "         [116., 115.,  94.],\n",
              "         [ 68.,  65.,  58.],\n",
              "         [ 27.,  26.,  26.]]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js3U-H9e8naq",
        "outputId": "f52898c9-ea88-41ad-a4d4-f7a2ce9f41c0"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6],\n",
              "       [9],\n",
              "       [9],\n",
              "       ...,\n",
              "       [9],\n",
              "       [1],\n",
              "       [1]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nefBMFijCNma"
      },
      "source": [
        "##**Converting the y values to categories**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF6Ib6_n8nYE"
      },
      "source": [
        "number_classes = 10"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_hxH-O48nVA",
        "outputId": "721e4875-3681-442e-d595-9ecfb3f0293f"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, number_classes)\n",
        "\n",
        "y_train\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxjkOpvA8nPm",
        "outputId": "136f84fe-d51a-4c68-fc20-d139b9e2874d"
      },
      "source": [
        "y_test = tf.keras.utils.to_categorical(y_test, number_classes)\n",
        "\n",
        "y_test"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OewYYHqMCpdW"
      },
      "source": [
        "##**Normalizing the x train and x test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFhoLozK8nM4"
      },
      "source": [
        "x_train = x_train/255\n",
        "x_test = x_test/255"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjjlT36h8nKh",
        "outputId": "529870e5-ceab-41e3-fd7f-cde4849c79eb"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.23137255, 0.24313725, 0.24705882],\n",
              "        [0.16862745, 0.18039216, 0.17647059],\n",
              "        [0.19607843, 0.18823529, 0.16862745],\n",
              "        ...,\n",
              "        [0.61960784, 0.51764706, 0.42352941],\n",
              "        [0.59607843, 0.49019608, 0.4       ],\n",
              "        [0.58039216, 0.48627451, 0.40392157]],\n",
              "\n",
              "       [[0.0627451 , 0.07843137, 0.07843137],\n",
              "        [0.        , 0.        , 0.        ],\n",
              "        [0.07058824, 0.03137255, 0.        ],\n",
              "        ...,\n",
              "        [0.48235294, 0.34509804, 0.21568627],\n",
              "        [0.46666667, 0.3254902 , 0.19607843],\n",
              "        [0.47843137, 0.34117647, 0.22352941]],\n",
              "\n",
              "       [[0.09803922, 0.09411765, 0.08235294],\n",
              "        [0.0627451 , 0.02745098, 0.        ],\n",
              "        [0.19215686, 0.10588235, 0.03137255],\n",
              "        ...,\n",
              "        [0.4627451 , 0.32941176, 0.19607843],\n",
              "        [0.47058824, 0.32941176, 0.19607843],\n",
              "        [0.42745098, 0.28627451, 0.16470588]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.81568627, 0.66666667, 0.37647059],\n",
              "        [0.78823529, 0.6       , 0.13333333],\n",
              "        [0.77647059, 0.63137255, 0.10196078],\n",
              "        ...,\n",
              "        [0.62745098, 0.52156863, 0.2745098 ],\n",
              "        [0.21960784, 0.12156863, 0.02745098],\n",
              "        [0.20784314, 0.13333333, 0.07843137]],\n",
              "\n",
              "       [[0.70588235, 0.54509804, 0.37647059],\n",
              "        [0.67843137, 0.48235294, 0.16470588],\n",
              "        [0.72941176, 0.56470588, 0.11764706],\n",
              "        ...,\n",
              "        [0.72156863, 0.58039216, 0.36862745],\n",
              "        [0.38039216, 0.24313725, 0.13333333],\n",
              "        [0.3254902 , 0.20784314, 0.13333333]],\n",
              "\n",
              "       [[0.69411765, 0.56470588, 0.45490196],\n",
              "        [0.65882353, 0.50588235, 0.36862745],\n",
              "        [0.70196078, 0.55686275, 0.34117647],\n",
              "        ...,\n",
              "        [0.84705882, 0.72156863, 0.54901961],\n",
              "        [0.59215686, 0.4627451 , 0.32941176],\n",
              "        [0.48235294, 0.36078431, 0.28235294]]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fktDTnuJ8nHy",
        "outputId": "df346640-1c36-4c6e-beae-87596fed2659"
      },
      "source": [
        "x_train.shape, x_test.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 32, 32, 3), (10000, 32, 32, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PgENKpX8nE6",
        "outputId": "bdacaa42-432f-4c40-8988-bb1101fb07a0"
      },
      "source": [
        "# Building the Model\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "inputs = layers.Input(shape=(32,32,3))\n",
        "\n",
        "x = layers.Conv2D(filters= 32, kernel_size= 3, activation= \"relu\")(inputs)\n",
        "x = layers.Conv2D(filters= 32, kernel_size= 3, activation= \"relu\")(x)\n",
        "x = layers.MaxPool2D(pool_size= 2)(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "x= layers.Conv2D(filters= 64, kernel_size= 3, activation= \"relu\")(x)\n",
        "x = layers.Conv2D(filters = 64, kernel_size= 3, activation= \"relu\")(x)\n",
        "x = layers.MaxPool2D(pool_size= 2)(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "x = layers.Dense(1100, activation= \"relu\")(x)\n",
        "x = layers.Dropout(.3)(x)\n",
        "x = layers.Dense(1100, activation= \"relu\")(x)\n",
        "\n",
        "outputs = layers.Dense(number_classes, activation= \"softmax\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compiling the Model\n",
        "\n",
        "model.compile(\n",
        "    loss = tf.keras.losses.categorical_crossentropy, \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate= 0.001),\n",
        "    metrics = [\"accuracy\"]\n",
        ")\n",
        "\n",
        "# fitting data to the model\n",
        "\n",
        "model.fit(x_train,y_train,\n",
        "          epochs = 100,\n",
        "          validation_data = (x_test,y_test)\n",
        "        )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 31s 11ms/step - loss: 1.5508 - accuracy: 0.4284 - val_loss: 1.2656 - val_accuracy: 0.5403\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.1881 - accuracy: 0.5777 - val_loss: 1.0197 - val_accuracy: 0.6444\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.0450 - accuracy: 0.6320 - val_loss: 0.9564 - val_accuracy: 0.6642\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.9416 - accuracy: 0.6672 - val_loss: 0.8675 - val_accuracy: 0.6949\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.8628 - accuracy: 0.6934 - val_loss: 0.8278 - val_accuracy: 0.7156\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.8114 - accuracy: 0.7166 - val_loss: 0.8128 - val_accuracy: 0.7204\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7641 - accuracy: 0.7306 - val_loss: 0.7693 - val_accuracy: 0.7371\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.7259 - accuracy: 0.7449 - val_loss: 0.7517 - val_accuracy: 0.7417\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.6916 - accuracy: 0.7560 - val_loss: 0.7332 - val_accuracy: 0.7491\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.6661 - accuracy: 0.7653 - val_loss: 0.7396 - val_accuracy: 0.7480\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.6416 - accuracy: 0.7733 - val_loss: 0.7344 - val_accuracy: 0.7504\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.6253 - accuracy: 0.7793 - val_loss: 0.7358 - val_accuracy: 0.7527\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.5988 - accuracy: 0.7891 - val_loss: 0.7925 - val_accuracy: 0.7388\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.5895 - accuracy: 0.7913 - val_loss: 0.7629 - val_accuracy: 0.7466\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 19s 12ms/step - loss: 0.5706 - accuracy: 0.7968 - val_loss: 0.7227 - val_accuracy: 0.7576\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.5485 - accuracy: 0.8061 - val_loss: 0.7492 - val_accuracy: 0.7523\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.5450 - accuracy: 0.8096 - val_loss: 0.7371 - val_accuracy: 0.7575\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.5324 - accuracy: 0.8126 - val_loss: 0.6983 - val_accuracy: 0.7678\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.5189 - accuracy: 0.8177 - val_loss: 0.7192 - val_accuracy: 0.7631\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.5048 - accuracy: 0.8214 - val_loss: 0.6980 - val_accuracy: 0.7704\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.4974 - accuracy: 0.8246 - val_loss: 0.7286 - val_accuracy: 0.7664\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.4933 - accuracy: 0.8278 - val_loss: 0.7069 - val_accuracy: 0.7670\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.4829 - accuracy: 0.8306 - val_loss: 0.7576 - val_accuracy: 0.7585\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.4751 - accuracy: 0.8329 - val_loss: 0.7317 - val_accuracy: 0.7634\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.4712 - accuracy: 0.8366 - val_loss: 0.7171 - val_accuracy: 0.7635\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.4633 - accuracy: 0.8385 - val_loss: 0.7370 - val_accuracy: 0.7641\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.4518 - accuracy: 0.8420 - val_loss: 0.7144 - val_accuracy: 0.7673\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.4589 - accuracy: 0.8401 - val_loss: 0.7146 - val_accuracy: 0.7685\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.4401 - accuracy: 0.8470 - val_loss: 0.7303 - val_accuracy: 0.7700\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.4426 - accuracy: 0.8474 - val_loss: 0.7279 - val_accuracy: 0.7651\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.4306 - accuracy: 0.8521 - val_loss: 0.7434 - val_accuracy: 0.7620\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.4338 - accuracy: 0.8490 - val_loss: 0.7433 - val_accuracy: 0.7633\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.4216 - accuracy: 0.8553 - val_loss: 0.7248 - val_accuracy: 0.7669\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.4160 - accuracy: 0.8579 - val_loss: 0.7217 - val_accuracy: 0.7746\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.4152 - accuracy: 0.8568 - val_loss: 0.7050 - val_accuracy: 0.7759\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.4013 - accuracy: 0.8613 - val_loss: 0.7449 - val_accuracy: 0.7668\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.4096 - accuracy: 0.8587 - val_loss: 0.7501 - val_accuracy: 0.7712\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.4038 - accuracy: 0.8620 - val_loss: 0.7394 - val_accuracy: 0.7661\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.4050 - accuracy: 0.8615 - val_loss: 0.7431 - val_accuracy: 0.7692\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3977 - accuracy: 0.8635 - val_loss: 0.7211 - val_accuracy: 0.7736\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3875 - accuracy: 0.8671 - val_loss: 0.7594 - val_accuracy: 0.7730\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3927 - accuracy: 0.8657 - val_loss: 0.7264 - val_accuracy: 0.7794\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3908 - accuracy: 0.8666 - val_loss: 0.7333 - val_accuracy: 0.7650\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3728 - accuracy: 0.8740 - val_loss: 0.7786 - val_accuracy: 0.7586\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3773 - accuracy: 0.8697 - val_loss: 0.7447 - val_accuracy: 0.7702\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3746 - accuracy: 0.8734 - val_loss: 0.7997 - val_accuracy: 0.7609\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3701 - accuracy: 0.8752 - val_loss: 0.7622 - val_accuracy: 0.7693\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3761 - accuracy: 0.8733 - val_loss: 0.7480 - val_accuracy: 0.7676\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3677 - accuracy: 0.8753 - val_loss: 0.7489 - val_accuracy: 0.7689\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3545 - accuracy: 0.8805 - val_loss: 0.7527 - val_accuracy: 0.7708\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3565 - accuracy: 0.8798 - val_loss: 0.7307 - val_accuracy: 0.7721\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3516 - accuracy: 0.8806 - val_loss: 0.7662 - val_accuracy: 0.7660\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3532 - accuracy: 0.8817 - val_loss: 0.7925 - val_accuracy: 0.7634\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3549 - accuracy: 0.8817 - val_loss: 0.7563 - val_accuracy: 0.7688\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3488 - accuracy: 0.8821 - val_loss: 0.7417 - val_accuracy: 0.7692\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3450 - accuracy: 0.8867 - val_loss: 0.7821 - val_accuracy: 0.7702\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3471 - accuracy: 0.8845 - val_loss: 0.7224 - val_accuracy: 0.7747\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3426 - accuracy: 0.8860 - val_loss: 0.7961 - val_accuracy: 0.7698\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3480 - accuracy: 0.8843 - val_loss: 0.7625 - val_accuracy: 0.7701\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3299 - accuracy: 0.8890 - val_loss: 0.7741 - val_accuracy: 0.7680\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3402 - accuracy: 0.8875 - val_loss: 0.7940 - val_accuracy: 0.7701\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3359 - accuracy: 0.8878 - val_loss: 0.7798 - val_accuracy: 0.7667\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 0.3296 - accuracy: 0.8890 - val_loss: 0.7677 - val_accuracy: 0.7606\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3352 - accuracy: 0.8868 - val_loss: 0.7569 - val_accuracy: 0.7701\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3345 - accuracy: 0.8888 - val_loss: 0.7392 - val_accuracy: 0.7841\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3297 - accuracy: 0.8899 - val_loss: 0.7604 - val_accuracy: 0.7695\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3309 - accuracy: 0.8905 - val_loss: 0.7618 - val_accuracy: 0.7699\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3265 - accuracy: 0.8935 - val_loss: 0.7904 - val_accuracy: 0.7715\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3233 - accuracy: 0.8913 - val_loss: 0.7444 - val_accuracy: 0.7709\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3278 - accuracy: 0.8910 - val_loss: 0.7297 - val_accuracy: 0.7780\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3224 - accuracy: 0.8941 - val_loss: 0.7715 - val_accuracy: 0.7648\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3117 - accuracy: 0.8971 - val_loss: 0.7484 - val_accuracy: 0.7743\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3209 - accuracy: 0.8919 - val_loss: 0.7770 - val_accuracy: 0.7693\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3160 - accuracy: 0.8972 - val_loss: 0.7684 - val_accuracy: 0.7738\n",
            "Epoch 75/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3099 - accuracy: 0.8973 - val_loss: 0.7745 - val_accuracy: 0.7790\n",
            "Epoch 76/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3155 - accuracy: 0.8953 - val_loss: 0.8045 - val_accuracy: 0.7685\n",
            "Epoch 77/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3113 - accuracy: 0.8966 - val_loss: 0.7496 - val_accuracy: 0.7821\n",
            "Epoch 78/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3054 - accuracy: 0.9000 - val_loss: 0.7737 - val_accuracy: 0.7736\n",
            "Epoch 79/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3070 - accuracy: 0.8992 - val_loss: 0.7957 - val_accuracy: 0.7714\n",
            "Epoch 80/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3062 - accuracy: 0.8990 - val_loss: 0.7763 - val_accuracy: 0.7774\n",
            "Epoch 81/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3069 - accuracy: 0.8996 - val_loss: 0.7989 - val_accuracy: 0.7733\n",
            "Epoch 82/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3035 - accuracy: 0.9005 - val_loss: 0.7333 - val_accuracy: 0.7812\n",
            "Epoch 83/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2975 - accuracy: 0.9025 - val_loss: 0.7771 - val_accuracy: 0.7744\n",
            "Epoch 84/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.3034 - accuracy: 0.9024 - val_loss: 0.7861 - val_accuracy: 0.7692\n",
            "Epoch 85/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2999 - accuracy: 0.9014 - val_loss: 0.7909 - val_accuracy: 0.7711\n",
            "Epoch 86/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2940 - accuracy: 0.9031 - val_loss: 0.7986 - val_accuracy: 0.7810\n",
            "Epoch 87/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2908 - accuracy: 0.9046 - val_loss: 0.7928 - val_accuracy: 0.7766\n",
            "Epoch 88/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2891 - accuracy: 0.9066 - val_loss: 0.7591 - val_accuracy: 0.7748\n",
            "Epoch 89/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2784 - accuracy: 0.9086 - val_loss: 0.7723 - val_accuracy: 0.7748\n",
            "Epoch 90/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2858 - accuracy: 0.9083 - val_loss: 0.7815 - val_accuracy: 0.7704\n",
            "Epoch 91/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2796 - accuracy: 0.9080 - val_loss: 0.7950 - val_accuracy: 0.7735\n",
            "Epoch 92/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2812 - accuracy: 0.9089 - val_loss: 0.8093 - val_accuracy: 0.7628\n",
            "Epoch 93/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2799 - accuracy: 0.9088 - val_loss: 0.8305 - val_accuracy: 0.7740\n",
            "Epoch 94/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2829 - accuracy: 0.9090 - val_loss: 0.7680 - val_accuracy: 0.7734\n",
            "Epoch 95/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2798 - accuracy: 0.9082 - val_loss: 0.8318 - val_accuracy: 0.7714\n",
            "Epoch 96/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2801 - accuracy: 0.9089 - val_loss: 0.8380 - val_accuracy: 0.7725\n",
            "Epoch 97/100\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.2803 - accuracy: 0.9090 - val_loss: 0.7674 - val_accuracy: 0.7790\n",
            "Epoch 98/100\n",
            "1181/1563 [=====================>........] - ETA: 4s - loss: 0.2693 - accuracy: 0.9132"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z30hc3dG8m_Z",
        "outputId": "cc75c5a5-93aa-45f1-bb52-a3405f450161"
      },
      "source": [
        "model_evaluation = model.evaluate(x_test,y_test)\n",
        "\n",
        "print(f\"Model Accuracy: {model_evaluation[1] * 100: 0.2f}%\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 6ms/step - loss: 0.7921 - accuracy: 0.7789\n",
            "Model Accuracy:  77.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9U_kGQoIoLK"
      },
      "source": [
        "y_predict = model.predict(x_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U1mCRbbPeWQ",
        "outputId": "00ee56b9-a941-4f6b-9bbd-7ff26df4d4a9"
      },
      "source": [
        "y_pre = tf.squeeze(tf.round(y_predict))\n",
        "y_pre"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10000, 10), dtype=float32, numpy=\n",
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlMb6zuHJbBI",
        "outputId": "6b810e31-484a-42c5-fa73-070e02fdaf0a"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test,y_pre))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.75      0.80      1000\n",
            "           1       0.94      0.84      0.89      1000\n",
            "           2       0.76      0.64      0.69      1000\n",
            "           3       0.66      0.52      0.58      1000\n",
            "           4       0.78      0.75      0.77      1000\n",
            "           5       0.76      0.65      0.70      1000\n",
            "           6       0.77      0.87      0.82      1000\n",
            "           7       0.84      0.81      0.83      1000\n",
            "           8       0.92      0.81      0.86      1000\n",
            "           9       0.87      0.85      0.86      1000\n",
            "\n",
            "   micro avg       0.82      0.75      0.78     10000\n",
            "   macro avg       0.81      0.75      0.78     10000\n",
            "weighted avg       0.81      0.75      0.78     10000\n",
            " samples avg       0.75      0.75      0.75     10000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}